import gc
import heapq
import json
import logging
import time
from typing import Callable, List, Optional, Tuple

import dspy
import torch
import tqdm

from src.dataset.beir.classes import RetrievalDataset
from src.evaluation.utils import convert_dict_results_to_lists, merge_list_of_retrieval_results
from src.generators.dspy_generator import DSPYGeneratorWrapper, Rerank, RerankTwoPassages
from src.rerankers.base_reranker import BaseReranker
from src.utils import ResultDict

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)


def basic_comparator_logic(_input: str) -> Tuple[bool, bool]:
    """Comparator logic: how to evaluate a string response to the question "Which article is more relevant to the query"

    :param _input: String generated by generator which was instructed to specify if 1st or 2nd article is more relevant
    :return: False if article 1 is more relevant than article 2. True otherwise. Second value of the tuple is True if
    the response was parsed successfully. False otherwise.
    """
    try:
        output_json = json.loads(_input)
        if output_json == {"passage": 1}:
            return False, True
        if output_json == {"passage": 2}:
            return True, True
        logger.warning(f"Got unexpected generator output for comparison: {_input}")
        return False, False
    except json.JSONDecodeError:
        logger.warning(f"JSON parsing. Got unexpected json format: {_input}")
        return False, False


class ComparatorLogic:
    """Class which is initialized with a function converting a string response from generator model to True or False
    indicating if article 1 is more or less relevant than article 2"""

    def __init__(self, logic_function: Callable[[str], bool] = None):
        self.logic_function = logic_function if logic_function is not None else basic_comparator_logic
        self.number_of_wrong_responses = 0

    def __call__(self, _input: str) -> bool:
        comparison_output, success = self.logic_function(_input)
        if not success:
            self.number_of_wrong_responses += 1
        return comparison_output


class LLMComparator:
    """Class encapsulating the whole reranking pipeline. Takes as input a generator (requires a 'query' function
    which takes as input a string and responses with a string). Afterwards it creates a DSPY rerank model enforcing the
    generator to respond in a standardized way. It also requires a Comparator logic object which describes how to
    interpret generator responses.
    """

    def __init__(self, generator_path: str, comparator_logic: ComparatorLogic = None):
        self.dspy_model = DSPYGeneratorWrapper(generator_path)
        dspy.settings.configure(lm=self.dspy_model, trace=[])
        rerank = Rerank(RerankTwoPassages)
        self.rerank = dspy.assert_transform_module(rerank, assertion_handler=dspy.backtrack_handler, max_backtracks=1)
        self.comparator_logic = ComparatorLogic() if comparator_logic is None else comparator_logic
        self.num_comparisons = 0

    def __call__(self, question: str, passage1: str, passage2: str) -> bool:
        """Takes in the question and two passages, and returns true if the first passage is less relevant to the
        question than the second

        :param question: Question for which we are reranking the passages
        :param passage1: First passage that we are comparing the relevance of
        :param passage2: Second passage that we are comparing the relevance of
        :return: True if the first passage is less relevant than the second. False otherwise"""
        answer = self.rerank.forward(question, passage1, passage2)
        return self.comparator_logic(answer)


class PassageForComparison:
    """Class encapsulating a passage, with the question it is being reranked in respect to. It requires the __lt__
    function which tells if the passage is less relevant to the question than another passage."""

    def __init__(self, passage: str, passage_id: str, question: str, comparator: LLMComparator):
        self.passage = passage
        self.question = question
        self.comparator = comparator
        self.passage_id = passage_id

    def __str__(self):
        return self.passage

    def __lt__(self, o):
        self.comparator.num_comparisons += 1
        return self.comparator(self.question, self.passage, o.passage)


class LLMComparisonReranker(BaseReranker):
    def __init__(self, generator_config_path: str, top_k: Optional[int] = None):
        logger.info(f"Loading generator from {generator_config_path}")

        self.top_k = top_k
        self.llm_comparator = LLMComparator(generator_config_path, None)

    def rerank_results(self, results: List[ResultDict], dataset: RetrievalDataset) -> List[ResultDict]:
        """
        Function reranks the results from the retriever. Overridden by subclasses.

        :param results: Dict of queries and their respective retrieved passages and scores
        :param dataset: RetrievalDataset object
        :return: New results in the same format as the input.
        """
        start_time = time.perf_counter()
        logger.info("Start reranking results with LLM Comparison Reranker.")

        final_results = {}
        results = merge_list_of_retrieval_results(results)
        v = convert_dict_results_to_lists(results, dataset)
        query_ids_list, queries_list, doc_ids_list, passages_list = v.query_ids, v.queries, v.doc_ids, v.passages
        i = 0
        for query_id, query, doc_ids, passages in tqdm.tqdm(
            zip(query_ids_list, queries_list, doc_ids_list, passages_list)
        ):
            query_start_time = time.perf_counter()
            comparison_passages = [
                PassageForComparison(passage=p, passage_id=p_id, question=query, comparator=self.llm_comparator)
                for p, p_id in zip(passages, doc_ids)
            ]
            top_n = self.top_k if self.top_k is not None else len(comparison_passages)
            results = heapq.nlargest(top_n, comparison_passages)
            curr_res_dict = {}
            for sc, r in enumerate(results):
                curr_res_dict[r.passage_id] = -sc
            final_results[query_id] = curr_res_dict
            i += 1
            logger.debug(
                f"Finished reranking query {i}.\n"
                f"Number of comparisons: {self.llm_comparator.num_comparisons}.\n"
                f"Elapsed time {time.perf_counter() - query_start_time} seconds."
            )
            self.llm_comparator.num_comparisons = 0

        logger.info(f"Finished LLM Comparison reranking. Elapsed time: {time.perf_counter() - start_time} seconds.")
        return [final_results]

    def set_automatic_batch_size(self, number_of_passages: int) -> None:
        """
        Not necessary here, but present to satisfy the abstract method in the base class.
        """
        pass

    def __enter__(self):
        return self

    def __exit__(self, *args, **kwargs):
        logger.info("__exit__ Cleaning up LLM Comparison Reranker.")
        if self.llm_comparator:
            if self.llm_comparator.comparator_logic:
                if hasattr(self.llm_comparator.comparator_logic, "number_of_wrong_responses"):
                    logger.info(
                        f"Number of wrong format responses: "
                        f"{self.llm_comparator.comparator_logic.number_of_wrong_responses}"
                    )

            if hasattr(self.llm_comparator.rerank, "number_of_dspy_corrections"):
                logger.info(f"Number of dspy corrections: {self.llm_comparator.rerank.number_of_dspy_corrections}")

            self.llm_comparator.dspy_model.__exit__(*args, **kwargs)
            del self.llm_comparator
            self.llm_comparator = None

        gc.collect()
        torch.cuda.empty_cache()
